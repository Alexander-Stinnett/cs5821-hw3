USING GIVEN EXAMPLE PROMPTS

Using prompt: "Blizzard T. Husky is"

Pythia-14m (Standard): Blizzard T. Husky is a great-grandfather, and the first time, the first time, the second time, the

Pythia-31m (Standard): Blizzard T. Husky is a popular choice for the game. The game is a game that is played by the game

Pythia-70m-deduped: Blizzard T. Husky is a good example of the game. The game is a game that is played by a player

Pythia-160m-deduped: Blizzard T. Husky is a very good game, and I think it's a good game. I'm not sure

Pythia-410m-deduped: Blizzard T. Husky is a great player. I’m not sure if this is a good game, but I

Pythia-1b-deduped: Blizzard T. Husky is a great game, but it’s not a great game. It’s not a great game

Conclusion: I wouldn't say it's obvious that the bigger model is better, except for 14m being the least coherent, because they all don't make much sense.
You can tell that the difference between 14m mentioning grandfather but all higher parameter models mentioning game shows emergent behavior.
Once it starts referring to a game, it still doesn't have any real consistency. 
But it is pretty funny that it seems to be mentioning Blizzard the game company, and its emergent behavior is that it realizes Blizzard makes bad games. 


Using prompt: "Write me a poem about Blizzard T. Husky"

Pythia-14m (Standard): Write me a poem about Blizzard T. Husky, and the first poem, the first poem, and the poem, the poem, the poem,

Pythia-31m (Standard): Write me a poem about Blizzard T. Husky, and I was a poet who was a poet who was a poet who was a poet who was

Pythia-70m-deduped: Write me a poem about Blizzard T. Husky, a writer who has been a writer for the New York Times since 2000, and who has been

Pythia-160m-deduped: Write me a poem about Blizzard T. Husky, and I’m not sure I’ll ever get to see it again.

Pythia-410m-deduped: Write me a poem about Blizzard T. Husky. I’ve been a fan of the Blizzard T. Husky, and I

Pythia-1b-deduped: Write me a poem about Blizzard T. Husky, the guy who wrote the song "The Little Mermaid" and the one who wrote the

Conclusion: It's easier to tell here that the bigger models are better.
As the parameters increase, we see fewer repetition, and eventually in the 1b model we see a reference to literature
which is possibly due to "poem" is mentioned. We can definitely tell that it is not instruction tuned.


Using prompt: "Here are the top 10 reasons to go to Michigan Tech: 1. "

Pythia-14m (Standard): Here are the top 10 reasons to go to Michigan Tech: 1. The first time I was in the first time I was in the second time I was in

Pythia-31m (Standard): Here are the top 10 reasons to go to Michigan Tech: 1. 

2.1.1.2.2.2.2.2.2.2

Pythia-70m-deduped: Here are the top 10 reasons to go to Michigan Tech: 1. 
2.
3.
4.
5.
6.

Pythia-160m-deduped: Here are the top 10 reasons to go to Michigan Tech: 1. 
1. The top 10 reasons to go to Michigan Tech: 1. The top 10 reasons to

Pythia-410m-deduped: Here are the top 10 reasons to go to Michigan Tech: 1. 
2.
3.
4.
5.
6.

Pythia-1b-deduped: Here are the top 10 reasons to go to Michigan Tech: 1. 
2. The University of Michigan is a great place to start.
3. The University

Conclusion: I think that, generally, the bigger models perform better. 
However, we see repeated behavior between smaller and bigger models.
14m repeats based off of 1.
31m creates a bizarre "list" of numbers.
70m created an actual ordered list, although there's nothing in it.
160m repeats itself, similar to 14m.
410m creates a list, identical to 70m.
1b Actually fills the list with SOMETHING!


USING OUR PROMPTS


Using prompt: "I really want some "

Pythia-14m (Standard): I really want some stuff that you want to do this. I don't think it's a lot of things

Pythia-31m (Standard): I really want some of the most important things I have to do. I have been a lot of work for

Pythia-70m-deduped: I really want some of the things I’ve been doing for years. I’ve been doing a lot of things

Pythia-160m-deduped: I really want some of the things I've done to get me to do. I'm not sure I'm going to

Pythia-410m-deduped: I really want some of the best of the best. I’ve been a fan of the new and improved

Pythia-1b-deduped: I really want some help with this. I have a few questions that I would like to ask you. 1

Pythia-1.4b-deduped: I really want some help with this. I'm not sure if I can do this or not. A:

Conclusion: The models with higher parameter seem to be more coherent.


Using prompt: "Write lyrics to a song about the moon"

Pythia-14m (Standard): Write lyrics to a song about the moon. The moon is a song of the moon, which is a song of the moon,

Pythia-31m (Standard): Write lyrics to a song about the moon. The song was released on the album, and the song was released on the album.

Pythia-70m-deduped: Write lyrics to a song about the moon, and the song is a song about the moon. The song is a song about the

Pythia-160m-deduped: Write lyrics to a song about the moon, and the song is a song about the moon. The song is a song about the

Pythia-410m-deduped: Write lyrics to a song about the moon. The song is a song about the moon. It is a song about the moon.

Pythia-1b-deduped: Write lyrics to a song about the moon, and the moon is the only place in the world where you can see the moon.

Pythia-1.4b-deduped: Write lyrics to a song about the moon and the stars. I’m not sure if I’m going to be able to

Conclusion: The models with fewer parameters repeat themselves, while the final model admitted defeat. 


Using prompt: "The top ten things to do in New York are: 1. "

Pythia-14m (Standard): The top ten things to do in New York are: 1. I'm not sure I'm not sure I'm not sure I'm not sure I'm

Pythia-31m (Standard): The top ten things to do in New York are: 1. The first thing that is, I think, is that the first thing that is, I think

Pythia-70m-deduped: The top ten things to do in New York are: 1. 
(1) The top ten things to do in New York City.
(2) The

Pythia-160m-deduped: The top ten things to do in New York are: 1. 
2. The first thing to do in New York is to get a new one.
3

Pythia-410m-deduped: The top ten things to do in New York are: 1. 
2.
3.
4.
5.
6.

Pythia-1b-deduped: The top ten things to do in New York are: 1. 
2.
3.
4.
5.
6.

Pythia-1.4b-deduped: The top ten things to do in New York are: 1. 
2. Get a job.
3. Get a job.
4. Get

Conclusion: The models with higher parameters started to create lists, and the final model actually filled the list but with repetition. 


OVERALL CONCLUSION: It is not ALWAYS obvious to tell which model is bigger as some model behavior is repeated
between fewer and greater parameter models. However, the difference between the fewest parameter models and 
the greatest parameter models is obvious. The fewest paramter models generally output repetitive text and are less 
coherent than models with more parameters.
