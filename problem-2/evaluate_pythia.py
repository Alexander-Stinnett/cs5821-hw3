# Code pulled from https://github.com/EleutherAI/pythia
from transformers import GPTNeoXForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling
from datasets import load_dataset
from torch.utils.data import DataLoader
import torch
from tqdm import tqdm
import math

param_choice = "1.4b-deduped"
model_choice = "EleutherAI/pythia-" + param_choice
cache_dir_choice = "./pythia-" + param_choice + "/step3000"


model = GPTNeoXForCausalLM.from_pretrained(
    model_choice,
    revision="step3000",
    cache_dir=cache_dir_choice,
)
tokenizer = AutoTokenizer.from_pretrained(
    model_choice,
    revision="step3000",
    cache_dir=cache_dir_choice,
)

# Code generated by ChatGPT
tokenizer.pad_token = tokenizer.eos_token  # ensure valid pad token

# Load and tokenize dataset
dataset = load_dataset("zaibutcooler/mini-webtext", split="train")

def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True, max_length=512)

tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

# DataLoader with dynamic padding
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
loader = DataLoader(tokenized, batch_size=8, shuffle=False, collate_fn=collator, num_workers=0)

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Evaluate average loss per token
total_nll = 0.0      # sum of (loss * number of tokens)
total_tokens = 0

with torch.no_grad():
    for batch in tqdm(loader, desc="Evaluating"):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        # count non-masked tokens
        n_tokens = (batch["labels"] != -100).sum().item()
        total_nll += loss.item() * n_tokens
        total_tokens += n_tokens

avg_loss = total_nll / total_tokens
ppl = math.exp(avg_loss)

print(f"Pythia-{param_choice} Average loss per token: {avg_loss:.4f}")
print(f"Pythia-{param_choice} Perplexity: {ppl:.2f}")
